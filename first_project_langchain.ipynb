{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Index**\n",
    "1. Load llm\n",
    "2. Chain\n",
    "3. Simple Sequential Chain\n",
    "4. Sequiential Chain\n",
    "5. Agents\n",
    "6. Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from secret_key import openapi_key\n",
    "os.environ[\"OPENAI_API_KEY\"] = openapi_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Help to use openai llm\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm  = OpenAI(temperature = 0.8)\n",
    "name = llm('who is Bhavesh ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to open a restaurant for Indian food . Suggest a fancy name for it'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Help in creating a prompt which can be feed to model\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food . Suggest a fancy name for it\"\n",
    ")\n",
    "\n",
    "prompt_template_name.format(cuisine = 'Indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Spice Odyssey\"'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Langchain combine llm and prompt using chain object \n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt = prompt_template_name)\n",
    "\n",
    "chain.run('Indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential chain it used to pass the previous output of llm as input again to it \n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template_name = PromptTemplate(input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food . Suggest a fancy name for it\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt = prompt_template_name)\n",
    "\n",
    "prompt_template_items = PromptTemplate(input_variables=['restaurant_name'],\n",
    "                                       template= \"\"\"Suggest some menu items for {restaurant_name} for each name, return in format of dictionary\"\"\")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt = prompt_template_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"Sizzling Chicken Tikka Masala\"\n",
      "- Chicken Tikka Masala: Marinated chicken cooked in a creamy tomato sauce with a blend of spices, served sizzling hot.\n",
      "- Garlic Naan: Traditional Indian flatbread brushed with garlic butter and cooked in a tandoor oven.\n",
      "- Vegetable Samosas: Crispy pastry filled with spiced potatoes and mixed vegetables.\n",
      "- Mango Lassi: Refreshing yogurt-based drink with a hint of mango.\n",
      "\n",
      "2. \"Spicy Lamb Vindaloo\"\n",
      "- Lamb Vindaloo: Tender chunks of lamb cooked in a spicy and tangy tomato-based sauce with a mix of traditional Indian spices.\n",
      "- Basmati Rice: Fragrant long-grain rice cooked with aromatic spices.\n",
      "- Onion Bhaji: Fried fritters made with sliced onions, gram flour, and spices.\n",
      "- Spiced Chai: Traditional Indian black tea infused with a blend of warm spices.\n",
      "\n",
      "3. \"Paneer Tikka Masala\"\n",
      "- Paneer Tikka Masala: Cubes of paneer (Indian cottage cheese) cooked in a rich and creamy tomato sauce with a masala spice blend.\n",
      "- Garlic and Coriander Naan: Naan bread flavored with fresh garlic and coriander.\n",
      "- A\n"
     ]
    }
   ],
   "source": [
    "print(food_items_chain.run('Spice Palace'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. \"Royal Butter Chicken\" - Tender chicken cooked in a rich and creamy tomato butter sauce, served with basmati rice and naan bread.\n",
      "2. \"Saffron Lamb Korma\" - Slow-cooked lamb in a fragrant saffron-infused curry sauce, served with naan bread and cumin rice.\n",
      "3. \"Vegetable Biryani\" - Aromatic basmati rice cooked with mixed vegetables and spices, topped with fried onions and served with raita.\n",
      "4. \"Tandoori Shrimp Masala\" - Marinated shrimp cooked in a clay oven and finished in a creamy masala sauce, served with basmati rice and garlic naan bread.\n",
      "5. \"Paneer Makhani\" - Cubes of paneer cheese cooked in a creamy tomato and cashew nut sauce, served with naan bread and cumin rice.\n",
      "6. \"Chicken Tikka Masala\" - Grilled chicken pieces cooked in a spicy tomato and onion gravy, served with basmati rice and garlic butter naan bread.\n",
      "7. \"Dal Maharaja\" - Slow-cooked lentils with spices, served with basmati rice, naan bread, and a side of pickles and chut\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "chain = SimpleSequentialChain(chains = [name_chain,food_items_chain])\n",
    "\n",
    "response = chain.run('Indian')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential chain it used to pass the previous output of llm as input again to it \n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "llm  = OpenAI(temperature = 0.8)\n",
    "prompt_template_name = PromptTemplate(input_variables = ['cuisine'],\n",
    "    template = \"I want to open a restaurant for {cuisine} food . Suggest a fancy name for it\"\n",
    ")\n",
    "\n",
    "name_chain = LLMChain(llm=llm, prompt = prompt_template_name, output_key = \"restaurant_name\")\n",
    "\n",
    "llm  = OpenAI(temperature = 0.8)\n",
    "prompt_template_items = PromptTemplate(input_variables=['restaurant_name'],\n",
    "                                       template= \"\"\"Suggest some menu items for {restaurant_name} for each name.\"\"\")\n",
    "\n",
    "food_items_chain = LLMChain(llm=llm, prompt = prompt_template_items,output_key = \"menu_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\NLP\\LLM\\Langchain\\langchain_env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:141: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cuisine': 'Indian', 'restaurant_name': '\\n\\n\"Maharaja\\'s Palace: Fine Indian Cuisine\"', 'menu_items': '\\n\\n1. Chicken Tikka Masala\\n2. Lamb Vindaloo\\n3. Vegetable Biryani\\n4. Tandoori Salmon\\n5. Palak Paneer\\n6. Butter Chicken\\n7. Dal Makhani\\n8. Aloo Gobi\\n9. Rogan Josh (lamb curry)\\n10. Baingan Bharta (roasted eggplant dish)\\n11. Samosas (potato and pea filled pastries)\\n12. Naan bread with garlic and cilantro\\n13. Mango Lassi (yogurt-based drink)\\n14. Gulab Jamun (syrup-soaked dumplings)\\n15. Kheer (rice pudding)'}\n"
     ]
    }
   ],
   "source": [
    "# Sequntial chain help use to see intermediate output as well\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "chain = SequentialChain(chains = [name_chain,food_items_chain],\n",
    "                        input_variables = ['cuisine'],\n",
    "                        output_variables = [\"restaurant_name\",\"menu_items\"])\n",
    "\n",
    "\n",
    "response = chain({'cuisine':'Indian'})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AGENTS**\n",
    "\n",
    "Separate plugins which can be used by llm to anser the questions for which it don't have knowledge for  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The United States won the most medals at the 2024 Summer Olympics, with 40 gold medals and 126 medals in total. The most decorated athlete at the 2024 Olympics was Sir Jason Kenny from Great Britain, who won seven gold medals and nine overall in track cycling.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import  AgentType, initialize_agent, load_tools\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "#Loadding tools which we want our llm to use\n",
    "tools = load_tools(['wikipedia','llm-math'],llm=llm)\n",
    "\n",
    "#intiallizing agents with tools, llm and method it should use\n",
    "agent = initialize_agent(tools,\n",
    "                 llm,\n",
    "                 agent = AgentType.ZERO_SHOT_REACT_DESCRIPTION)\n",
    "\n",
    "agent.run(\"who win most medal in paris olmpyic? What is his right now in 2024?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory**\n",
    "Remembering of converstation is required in case of customer support etc. Chain do not have memory hence we need toapply it separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Config',\n",
       " 'InputType',\n",
       " 'OutputType',\n",
       " '__abstractmethods__',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__class_vars__',\n",
       " '__config__',\n",
       " '__custom_root_type__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__exclude_fields__',\n",
       " '__fields__',\n",
       " '__fields_set__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__get_validators__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__include_fields__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__json_encoder__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__or__',\n",
       " '__orig_bases__',\n",
       " '__parameters__',\n",
       " '__post_root_validators__',\n",
       " '__pre_root_validators__',\n",
       " '__pretty__',\n",
       " '__private_attributes__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__repr_args__',\n",
       " '__repr_name__',\n",
       " '__repr_str__',\n",
       " '__rich_repr__',\n",
       " '__ror__',\n",
       " '__schema_cache__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__signature__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__try_update_forward_refs__',\n",
       " '__validators__',\n",
       " '__weakref__',\n",
       " '_abatch_with_config',\n",
       " '_abc_impl',\n",
       " '_acall',\n",
       " '_acall_with_config',\n",
       " '_atransform_stream_with_config',\n",
       " '_batch_with_config',\n",
       " '_calculate_keys',\n",
       " '_call',\n",
       " '_call_with_config',\n",
       " '_chain_type',\n",
       " '_copy_and_set_values',\n",
       " '_decompose_class',\n",
       " '_enforce_dict_if_root',\n",
       " '_get_num_tokens',\n",
       " '_get_value',\n",
       " '_init_private_attributes',\n",
       " '_is_protocol',\n",
       " '_iter',\n",
       " '_parse_generation',\n",
       " '_run_output_key',\n",
       " '_transform_stream_with_config',\n",
       " '_validate_inputs',\n",
       " '_validate_outputs',\n",
       " 'aapply',\n",
       " 'aapply_and_parse',\n",
       " 'abatch',\n",
       " 'abatch_as_completed',\n",
       " 'acall',\n",
       " 'agenerate',\n",
       " 'ainvoke',\n",
       " 'apply',\n",
       " 'apply_and_parse',\n",
       " 'apredict',\n",
       " 'apredict_and_parse',\n",
       " 'aprep_inputs',\n",
       " 'aprep_outputs',\n",
       " 'aprep_prompts',\n",
       " 'arun',\n",
       " 'as_tool',\n",
       " 'assign',\n",
       " 'astream',\n",
       " 'astream_events',\n",
       " 'astream_log',\n",
       " 'atransform',\n",
       " 'batch',\n",
       " 'batch_as_completed',\n",
       " 'bind',\n",
       " 'callback_manager',\n",
       " 'callbacks',\n",
       " 'config_schema',\n",
       " 'config_specs',\n",
       " 'configurable_alternatives',\n",
       " 'configurable_fields',\n",
       " 'construct',\n",
       " 'copy',\n",
       " 'create_outputs',\n",
       " 'dict',\n",
       " 'from_orm',\n",
       " 'from_string',\n",
       " 'generate',\n",
       " 'get_graph',\n",
       " 'get_input_schema',\n",
       " 'get_lc_namespace',\n",
       " 'get_name',\n",
       " 'get_output_schema',\n",
       " 'get_prompts',\n",
       " 'input_keys',\n",
       " 'input_schema',\n",
       " 'invoke',\n",
       " 'is_lc_serializable',\n",
       " 'json',\n",
       " 'lc_attributes',\n",
       " 'lc_id',\n",
       " 'lc_secrets',\n",
       " 'llm',\n",
       " 'llm_kwargs',\n",
       " 'map',\n",
       " 'memory',\n",
       " 'metadata',\n",
       " 'name',\n",
       " 'output_key',\n",
       " 'output_keys',\n",
       " 'output_parser',\n",
       " 'output_schema',\n",
       " 'parse_file',\n",
       " 'parse_obj',\n",
       " 'parse_raw',\n",
       " 'pick',\n",
       " 'pipe',\n",
       " 'predict',\n",
       " 'predict_and_parse',\n",
       " 'prep_inputs',\n",
       " 'prep_outputs',\n",
       " 'prep_prompts',\n",
       " 'prompt',\n",
       " 'raise_callback_manager_deprecation',\n",
       " 'return_final_only',\n",
       " 'run',\n",
       " 'save',\n",
       " 'schema',\n",
       " 'schema_json',\n",
       " 'set_verbose',\n",
       " 'stream',\n",
       " 'tags',\n",
       " 'to_json',\n",
       " 'to_json_not_implemented',\n",
       " 'transform',\n",
       " 'update_forward_refs',\n",
       " 'validate',\n",
       " 'verbose',\n",
       " 'with_alisteners',\n",
       " 'with_config',\n",
       " 'with_fallbacks',\n",
       " 'with_listeners',\n",
       " 'with_retry',\n",
       " 'with_types']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chain.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\"Taj Mahal Spice Co.\"'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt = prompt_template_name,memory = memory)\n",
    "\n",
    "chain.run('Indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe Star-Spangled Bistro '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run('American')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=InMemoryChatMessageHistory(messages=[HumanMessage(content='Indian'), AIMessage(content='\\n\\n\"Indigo Spice\" '), HumanMessage(content='American'), AIMessage(content='\\n\\nThe Star-Spangled Bistro ')]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Indian\n",
      "AI: \n",
      "\n",
      "\"Indigo Spice\" \n",
      "Human: American\n",
      "AI: \n",
      "\n",
      "The Star-Spangled Bistro \n"
     ]
    }
   ],
   "source": [
    "print(chain.memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "# the conversation grow endlessly hence need to but some trimming value or upper limit. we will use Conversation chain to do this\n",
    "\n",
    "from langchain.chains import ConversationChain\n",
    "convo  = ConversationChain(llm = OpenAI(temperature=0.7),memory=memory)\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The last Cricket World Cup was won by England in 2019.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('who won last cricket world cup?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 11 '"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('what is 5+6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The captain of the winning team, England, is Eoin Morgan.'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('who is captian of winning team?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "{history}\n",
      "Human: {input}\n",
      "AI:\n"
     ]
    }
   ],
   "source": [
    "# setting up threshold\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "\n",
    "convo  = ConversationChain(llm = OpenAI(temperature=0.7),memory=memory)\n",
    "print(convo.prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The last cricket world cup was won by England in 2019. It was a historic win for England as it was their first time winning the world cup. The final match was played between England and New Zealand, and it was an intense and thrilling game. The match ended in a tie, but England was declared the winner based on the boundary count rule. It was a controversial finish, but England's team and fans were ecstatic with the win.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('who won last cricket world cup?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 5 + 6 is equal to 11.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('what is 5+6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I do not have enough information to answer that question accurately. However, if you are referring to a specific team, I may be able to provide you with the name of the captain. Can you give me more context?'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convo.run('who is captian of winning team?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
